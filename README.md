# MLOps Day 1: Foundations - Complete Experiment Tracking System

A comprehensive MLOps implementation demonstrating professional-grade experiment tracking, model versioning, and pipeline automation using MLFlow, Weights & Biases, and DVC.
<img width="1740" height="850" alt="Model_Results" src="https://github.com/user-attachments/assets/9183f2b1-9f2d-4d3e-afc1-c1055667bd38" />

<img width="1515" height="847" alt="Model_comparison_results" src="https://github.com/user-attachments/assets/b5385266-1f6c-479a-ae0a-cb0c42da1680" />

<img width="1843" height="752" alt="ML-pipeline-experiments_day1-charts" src="https://github.com/user-attachments/assets/06965895-8077-4d60-b2b9-bc2ec2fc3579" />

<img width="1844" height="704" alt="ML-pipeline-experiments_day1-models" src="https://github.com/user-attachments/assets/363ff04d-ac87-4997-b1f4-cd680b575456" />

## ğŸ¯ Project Overview

This project implements a complete MLOps foundation that demonstrates:

- **Experiment Tracking** with MLFlow and Weights & Biases
- **Model Versioning** and Registry management
- **Data Version Control** with DVC
- **Automated Pipeline** orchestration
- **Comprehensive Model Evaluation** with visualizations
- **Production-Ready** code structure

### ğŸ† Results Achieved

| Model | Test Accuracy | Precision | Recall | F1-Score |
|-------|---------------|-----------|---------|----------|
| **Logistic Regression** | **98.25%** | 98.28% | 98.25% | 98.25% |
| **SVM** | **98.25%** | 98.28% | 98.25% | 98.25% |
| **Random Forest** | **95.61%** | 95.74% | 95.61% | 95.63% |

## ğŸ“ Project Structure

```
mlops-day1-project/
â”œâ”€â”€ ğŸ“Š data/
â”‚   â”œâ”€â”€ raw/                    # Original datasets
â”‚   â””â”€â”€ processed/              # Cleaned and split data
â”œâ”€â”€ âš™ï¸ configs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mlflow_config.py        # MLFlow configuration
â”œâ”€â”€ ğŸ§  models/
â”‚   â”œâ”€â”€ random_forest_model.joblib
â”‚   â”œâ”€â”€ logistic_regression_model.joblib
â”‚   â”œâ”€â”€ svm_model.joblib
â”‚   â””â”€â”€ scaler.joblib           # Data preprocessing scaler
â”œâ”€â”€ ğŸ“ˆ metrics/
â”‚   â”œâ”€â”€ model_metrics.json      # Training metrics
â”‚   â””â”€â”€ evaluation_metrics.json # Evaluation results
â”œâ”€â”€ ğŸ“Š plots/
â”‚   â””â”€â”€ comprehensive_model_evaluation.png
â”œâ”€â”€ ğŸ“‹ reports/
â”‚   â””â”€â”€ detailed_evaluation_report.json
â”œâ”€â”€ ğŸ”§ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_pipeline.py        # Data processing pipeline
â”‚   â”œâ”€â”€ train_multiple_models.py # Model training
â”‚   â”œâ”€â”€ experiment_tracker.py   # MLFlow tracking utilities
â”‚   â”œâ”€â”€ ml_pipeline.py          # Core ML pipeline
â”‚   â””â”€â”€ wandb_integration.py    # W&B integration
â”œâ”€â”€ ğŸš€ mlruns/                  # MLFlow tracking store
â”œâ”€â”€ ğŸ“ dvc.yaml                 # DVC pipeline definition
â”œâ”€â”€ âš™ï¸ params.yaml              # Model hyperparameters
â”œâ”€â”€ ğŸ“‹ requirements.txt         # Python dependencies
â”œâ”€â”€ ğŸ”§ register_models.py       # Model registry setup
â”œâ”€â”€ ğŸ“Š enhanced_evaluation.py   # Comprehensive evaluation
â”œâ”€â”€ ğŸš€ run_pipeline.py          # Pipeline orchestration
â”œâ”€â”€ ğŸ§ª test_pipeline.py         # Testing framework
â””â”€â”€ ğŸ“– README.md               # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Git
- 4GB+ RAM recommended

### 1. Clone and Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd mlops-day1-project

# Create virtual environment
python -m venv mlops-env
source mlops-env/bin/activate  # On Windows: mlops-env\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Initialize MLFlow

```bash
# Start MLFlow UI (in separate terminal)
mlflow ui --host 0.0.0.0 --port 5000

# Access at: http://localhost:5000
```

### 3. Run the Complete Pipeline

```bash
# Option 1: Run individual steps
python src/data_pipeline.py
python src/train_multiple_models.py
python register_models.py
python enhanced_evaluation.py

# Option 2: Run automated pipeline
python run_pipeline.py
```

### 4. View Results

- **MLFlow UI**: http://localhost:5000
- **Model Metrics**: `metrics/model_metrics.json`
- **Evaluation Plots**: `plots/comprehensive_model_evaluation.png`
- **Detailed Reports**: `reports/detailed_evaluation_report.json`

## ğŸ”§ Core Components

### Data Pipeline (`src/data_pipeline.py`)

```python
# Automated data processing and splitting
pipeline = DataPipeline("breast_cancer")
df, metadata = pipeline.create_raw_data()        # Load dataset
X_train, X_test, y_train, y_test = pipeline.process_data()  # Split and save
```

**Features:**
- âœ… Automated dataset loading (Breast Cancer dataset)
- âœ… Train/test splitting with stratification
- âœ… Metadata tracking and validation
- âœ… Robust error handling

### Model Training (`src/train_multiple_models.py`)

```python
# Multi-model training with MLFlow tracking
models = ["random_forest", "logistic_regression", "svm"]
for model_type in models:
    model, metrics = train_model(model_type, params)
```

**Features:**
- âœ… Multiple algorithm support
- âœ… Hyperparameter configuration via `params.yaml`
- âœ… Comprehensive metrics tracking
- âœ… Model artifact versioning
- âœ… MLFlow experiment logging

### Model Registry (`register_models.py`)

```python
# Register models in MLFlow Model Registry
model_registry = setup_model_registry()
promote_best_model(client, model_registry)
```

**Features:**
- âœ… Automatic model registration
- âœ… Version management
- âœ… Stage transitions (Staging â†’ Production)
- âœ… Model comparison and promotion

### Enhanced Evaluation (`enhanced_evaluation.py`)

```python
# Comprehensive model evaluation
evaluator = EnhancedModelEvaluator()
results, y_test, features = evaluator.evaluate_all_models()
metrics_df = evaluator.create_comprehensive_visualizations(results, y_test, features)
```

**Features:**
- âœ… 9 different visualization types
- âœ… ROC curves and confusion matrices
- âœ… Feature importance analysis
- âœ… Model complexity vs performance
- âœ… Detailed JSON reports

## ğŸ“Š Experiment Tracking

### MLFlow Integration

- **Experiments**: All runs tracked in `ml-pipeline-experiments`
- **Parameters**: Model hyperparameters and dataset info
- **Metrics**: Training/test accuracy, precision, recall, F1-score
- **Artifacts**: Trained models, plots, and metadata
- **Model Registry**: Named, versioned models with stage management

### Weights & Biases (Optional)

```python
# W&B integration for advanced tracking
wandb_integration = WandBIntegration(project_name="mlops-day1")
wandb_integration.init_run(config=params, run_name="experiment_1")
```

### Data Version Control (DVC)

```yaml
# dvc.yaml - Pipeline definition
stages:
  prepare_data:
    cmd: python src/data_pipeline.py
    outs: [data/raw/, data/processed/]
  
  train_models:
    cmd: python src/train_multiple_models.py
    deps: [data/processed/]
    outs: [models/]
    metrics: [metrics/model_metrics.json]
```

## ğŸ¯ Model Performance

### Hyperparameter Configuration

```yaml
# params.yaml
models:
  random_forest:
    n_estimators: 100
    max_depth: 7
    min_samples_split: 5
    min_samples_leaf: 2
    random_state: 42
  
  logistic_regression:
    C: 1.0
    penalty: l2
    solver: liblinear
    max_iter: 1000
    random_state: 42
  
  svm:
    C: 1.0
    kernel: rbf
    random_state: 42
```

### Model Metrics

| Metric | Random Forest | Logistic Regression | SVM |
|--------|---------------|---------------------|-----|
| **Accuracy** | 95.61% | **98.25%** | **98.25%** |
| **Precision** | 95.74% | **98.28%** | **98.28%** |
| **Recall** | 95.61% | **98.25%** | **98.25%** |
| **F1-Score** | 95.63% | **98.25%** | **98.25%** |
| **Training Time** | ~0.15s | ~0.05s | ~0.08s |

## ğŸ” Advanced Features

### Automated Model Promotion

```python
# Best performing model automatically promoted to Production
best_model = max(models, key=lambda x: x['test_accuracy'])
client.transition_model_version_stage(
    name=best_model_name,
    version=best_model_version,
    stage="Production"
)
```

### Comprehensive Visualizations

The evaluation pipeline generates 9 different visualization types:

1. **ğŸ“Š Metrics Comparison** - Side-by-side performance comparison
2. **ğŸ† Accuracy Ranking** - Models ranked by performance
3. **ğŸ“ˆ ROC Curves** - Receiver Operating Characteristic analysis
4. **ğŸ” Confusion Matrices** - Per-model classification analysis
5. **ğŸŒŸ Feature Importance** - Random Forest feature analysis
6. **âš–ï¸ Complexity vs Performance** - Model trade-off analysis
7. **ğŸ“‹ Summary Statistics** - Comprehensive performance summary
8. **ğŸ“Š Class Distribution** - Dataset balance analysis
9. **ğŸ¯ Model Recommendations** - Best use-case scenarios

### Testing Framework

```python
# Automated testing pipeline
python test_pipeline.py

# Tests include:
# âœ… Data pipeline functionality
# âœ… MLFlow tracking verification
# âœ… Model training validation
# âœ… File output verification
```

## ğŸ› ï¸ Technology Stack

| Category | Technology | Purpose |
|----------|------------|---------|
| **ğŸ§  ML Framework** | scikit-learn | Model training and evaluation |
| **ğŸ“Š Experiment Tracking** | MLFlow | Experiment management and model registry |
| **ğŸŒ Advanced Tracking** | Weights & Biases | Enhanced monitoring and collaboration |
| **ğŸ“ Data Versioning** | DVC | Data and pipeline version control |
| **ğŸ“ˆ Visualization** | matplotlib, seaborn | Plot generation and analysis |
| **ğŸ”§ Data Processing** | pandas, numpy | Data manipulation and analysis |
| **ğŸ’¾ Model Persistence** | joblib | Model serialization |
| **âš™ï¸ Configuration** | YAML | Parameter management |

## ğŸ“š Usage Examples

### Training a Single Model

```python
from src.ml_pipeline import MLPipeline

# Initialize pipeline
pipeline = MLPipeline("breast_cancer")

# Train specific model
model, metrics = pipeline.train_model(
    model_type="random_forest",
    n_estimators=100,
    max_depth=7
)

print(f"Model accuracy: {metrics['test_accuracy']:.4f}")
```

### Comparing Models

```python
from enhanced_evaluation import EnhancedModelEvaluator

# Run comprehensive evaluation
evaluator = EnhancedModelEvaluator()
results, y_test, features = evaluator.evaluate_all_models()

# Generate visualizations
metrics_df = evaluator.create_comprehensive_visualizations(
    results, y_test, features
)
```

### Accessing MLFlow Data

```python
import mlflow
from mlflow.tracking import MlflowClient

# Initialize client
client = MlflowClient()

# Get best run
experiment = client.get_experiment_by_name("ml-pipeline-experiments")
runs = client.search_runs([experiment.experiment_id])
best_run = max(runs, key=lambda x: x.data.metrics.get('test_accuracy', 0))

print(f"Best accuracy: {best_run.data.metrics['test_accuracy']:.4f}")
```

## ğŸ§ª Testing

### Run All Tests

```bash
python test_pipeline.py
```

### Test Coverage

- âœ… Data pipeline functionality
- âœ… Model training workflow
- âœ… MLFlow tracking integration
- âœ… File output validation
- âœ… Error handling verification

## ğŸ“ˆ Monitoring and Alerts

### MLFlow Tracking

- **Real-time metrics**: Track training progress live
- **Parameter comparison**: Compare hyperparameter effects
- **Artifact storage**: Automatic model and plot storage
- **Version control**: Complete model lineage tracking

### Performance Monitoring

```python
# Monitor model performance over time
def monitor_model_drift():
    latest_metrics = load_latest_metrics()
    baseline_metrics = load_baseline_metrics()
    
    drift_threshold = 0.05
    accuracy_drift = abs(latest_metrics['accuracy'] - baseline_metrics['accuracy'])
    
    if accuracy_drift > drift_threshold:
        send_alert(f"Model drift detected: {accuracy_drift:.3f}")
```

## ğŸš€ Deployment Considerations

### Model Serving

```python
# Load production model
import mlflow.sklearn

model_uri = "models:/breast_cancer_logistic_regression/Production"
model = mlflow.sklearn.load_model(model_uri)

# Make predictions
predictions = model.predict(new_data)
```

### API Integration

```python
# FastAPI endpoint example
from fastapi import FastAPI
import mlflow.sklearn

app = FastAPI()
model = mlflow.sklearn.load_model("models:/best_model/Production")

@app.post("/predict")
def predict(features: List[float]):
    prediction = model.predict([features])
    return {"prediction": int(prediction[0])}
```

## ğŸ”’ Security and Compliance

### Data Privacy

- âœ… No sensitive data in version control
- âœ… Configurable data encryption
- âœ… Access control integration ready
- âœ… Audit trail through MLFlow

### Model Governance

- âœ… Model approval workflows
- âœ… Performance monitoring
- âœ… Bias detection capabilities
- âœ… Regulatory compliance tracking

## ğŸ¤ Contributing

### Development Setup

```bash
# Clone and setup development environment
git clone <repo-url>
cd mlops-day1-project
python -m venv dev-env
source dev-env/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Additional dev dependencies
```

### Code Standards

- **PEP 8** compliance for Python code
- **Type hints** for function signatures
- **Docstrings** for all classes and functions
- **Unit tests** for critical functionality


**Problem**: MLFlow UI not showing models
```bash
# Solution: Register models first
python register_models.py
```

**Problem**: Import errors
```bash
# Solution: Ensure proper Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

**Problem**: Memory issues with large datasets
```bash
# Solution: Reduce batch size or use data sampling
# Edit params.yaml to adjust model parameters
```

## ğŸ“ License

This project is licensed under the MIT License 

## ğŸ™ Acknowledgments

- **scikit-learn** team for excellent ML algorithms
- **MLFlow** community for experiment tracking tools
- **DVC** team for data version control
- **Weights & Biases** for advanced experiment monitoring

## ğŸ“Š Project Metrics

![GitHub last commit](https://img.shields.io/github/last-commit/username/mlops-day1-project)
![GitHub issues](https://img.shields.io/github/issues/username/mlops-day1-project)
![GitHub stars](https://img.shields.io/github/stars/username/mlops-day1-project)
![GitHub forks](https://img.shields.io/github/forks/username/mlops-day1-project)

---

**ğŸ¯ Ready for Production | ğŸ”§ Enterprise-Grade | ğŸ“Š Fully Tracked**

> This project demonstrates professional MLOps practices essential for senior ML engineering roles. It showcases end-to-end pipeline automation, comprehensive experiment tracking, and production-ready code architecture.

---

*Built with â¤ï¸ for the MLOps community*
